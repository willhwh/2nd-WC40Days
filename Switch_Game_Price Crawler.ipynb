{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filechecker(name):\n",
    "    data=pd.read_csv(name)\n",
    "    print('top 5 of',len(data),'rows from',name, 'data\\n',data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# single thread crawler\n",
    "- the first 3 pages of switch game price info. in hottest order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.412523031234741 secs spend to craw these 3 pages\n",
      "top 5 of 108 rows from sgp_first3.csv data\n",
      "                                           Name Discount                   Tag  \\\n",
      "0        Tales of Vesperia: Definitive Edition     -60%  Matches previous low   \n",
      "1  Mario + Rabbids Kingdom Battle Gold Edition     -75%  Matches previous low   \n",
      "2                      DRAGON BALL Xenoverse 2     -80%     Lowest price ever   \n",
      "3          Monster Hunter Generations Ultimate     -60%  Matches previous low   \n",
      "4                Trine 4: The Nightmare Prince     -60%     Lowest price ever   \n",
      "\n",
      "  Original_Price Discount_Price          Deadline  \n",
      "0         $49.99         $19.99  Sale ends May 26  \n",
      "1         $79.99         $19.99  Sale ends May 26  \n",
      "2         $49.99          $9.99  Sale ends May 26  \n",
      "3         $49.99         $19.99  Sale ends May 27  \n",
      "4         $29.99         $11.99  Sale ends May 26  \n"
     ]
    }
   ],
   "source": [
    "with open('sgp_first3.csv',\"a+\",encoding='utf-8') as file:\n",
    "            writer = csv.writer(file, delimiter=',')\n",
    "            writer.writerow(['Name','Discount','Tag','Original_Price','Discount_Price','Deadline'])\n",
    "\n",
    "\n",
    "\n",
    "main='https://www.dekudeals.com/'\n",
    "link_list=[]\n",
    "headers={'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36'}\n",
    "for i in range(1,4):\n",
    "    link_list.append(main+'hottest?page='+str(i))\n",
    "    \n",
    "startTime=time.time()\n",
    "for link in link_list:\n",
    "    response=requests.get(link,headers=headers)\n",
    "    soup=BeautifulSoup(response.text,'html.parser')\n",
    "    \n",
    "    games=soup.find_all('div',{'class':'cell'})\n",
    "    games\n",
    "    for game in games:\n",
    "        describe=game.find('div',{'class':'h6 name'}).text.strip()\n",
    "        name=[]\n",
    "        discount=[]\n",
    "        tag=[]\n",
    "        original_price=[]\n",
    "        discount_price=[]\n",
    "        deadline=[]\n",
    "        try:\n",
    "            describe.split('\\n')[0]\n",
    "            name=describe.split('\\n')[0]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            describe.split('\\n')[1]\n",
    "            discount=describe.split('\\n')[1]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            describe.split('\\n')[2]\n",
    "            tag=describe.split('\\n')[2]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            original_price=game.find('s',{'class':'text-muted'}).text.strip()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            discount_price=game.find('strong').text.strip()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            deadline=game.find('div',{'class':'w-100'}).find('small').text.strip()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        with open('sgp_first3.csv',\"a+\",encoding='utf-8') as file:\n",
    "            writer = csv.writer(file, delimiter=',')\n",
    "            writer.writerow([name,discount,tag,original_price,discount_price,deadline])\n",
    "            \n",
    "finishTime = time.time()\n",
    "print(finishTime - startTime,'secs spend to craw these 3 pages')\n",
    "filechecker('sgp_first3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use proxy\n",
    "- the number 4-6 pages of switch game price info. in hottest order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "300 proxy in pool\n",
      "3 useble ip: ['68.183.208.248:80', '80.211.210.39:8080', '149.28.104.226:3128']\n"
     ]
    }
   ],
   "source": [
    "# get three proxy from link:  https://free-proxy-list.net/ \n",
    "url='https://free-proxy-list.net/'\n",
    "r=requests.get(url)\n",
    "print(r)\n",
    "soup=BeautifulSoup(r.text,'html5lib')\n",
    "ip_lst=[]\n",
    "n=0\n",
    "for i in soup.find('tbody').find_all('tr'):\n",
    "    n=n+1\n",
    "    ip=i.find('td').text\n",
    "    port=i.find_all('td')[1].text\n",
    "    ip_lst.append(i.find('td').text+':'+port)\n",
    "print(len(ip_lst),'proxy in pool')\n",
    "\n",
    "n=0\n",
    "useble_lst=[]\n",
    "for ip in ip_lst:\n",
    "    if n<3:\n",
    "        try:\n",
    "            resp = requests.get('http://ip.filefab.com/index.php',proxies={'http': ip})\n",
    "            soup = BeautifulSoup(resp.text, 'html5lib')\n",
    "            useble_lst.append(ip)\n",
    "            n=n+1\n",
    "        except:\n",
    "            continue\n",
    "print('3 useble ip:',useble_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4789209365844727 secs spend to craw these 3 pages\n",
      "top 5 of 108 rows from sgp_4-6.csv data\n",
      "                                         Name Discount                   Tag  \\\n",
      "0  Starlink: Battle for Atlas – Starter Pack     -71%                    []   \n",
      "1                              Syberia 1 & 2     -66%  Matches previous low   \n",
      "2    DRAGON BALL FIGHTERZ - Ultimate Edition     -75%  Matches previous low   \n",
      "3                            Thief Simulator     -90%  Matches previous low   \n",
      "4                               Bomb Chicken     -50%  Matches previous low   \n",
      "\n",
      "  Original_Price Discount_Price               Deadline  \n",
      "0         $74.99         $21.97                     []  \n",
      "1         $34.99         $12.00  Sale ends in 15 hours  \n",
      "2        $109.99         $27.49       Sale ends May 26  \n",
      "3         $19.99          $1.99       Sale ends May 27  \n",
      "4         $14.99          $7.49       Sale ends May 24  \n"
     ]
    }
   ],
   "source": [
    "with open('sgp_4-6.csv',\"a+\",encoding='utf-8') as file:\n",
    "            writer = csv.writer(file, delimiter=',')\n",
    "            writer.writerow(['Name','Discount','Tag','Original_Price','Discount_Price','Deadline'])\n",
    "\n",
    "main='https://www.dekudeals.com/'\n",
    "link_list=[]\n",
    "headers={'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36'}\n",
    "for i in range(4,7):\n",
    "    link_list.append(main+'hottest?page='+str(i))\n",
    "    \n",
    "startTime=time.time()\n",
    "for link,ip in zip(link_list,useble_lst):\n",
    "    response=requests.get(link,headers=headers,proxies={'http': ip})\n",
    "    soup=BeautifulSoup(response.text,'html.parser')\n",
    "    \n",
    "    games=soup.find_all('div',{'class':'cell'})\n",
    "    games\n",
    "    for game in games:\n",
    "        describe=game.find('div',{'class':'h6 name'}).text.strip()\n",
    "        name=[]\n",
    "        discount=[]\n",
    "        tag=[]\n",
    "        original_price=[]\n",
    "        discount_price=[]\n",
    "        deadline=[]\n",
    "        try:\n",
    "            describe.split('\\n')[0]\n",
    "            name=describe.split('\\n')[0]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            describe.split('\\n')[1]\n",
    "            discount=describe.split('\\n')[1]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            describe.split('\\n')[2]\n",
    "            tag=describe.split('\\n')[2]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            original_price=game.find('s',{'class':'text-muted'}).text.strip()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            discount_price=game.find('strong').text.strip()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            deadline=game.find('div',{'class':'w-100'}).find('small').text.strip()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        with open('sgp_4-6.csv',\"a+\",encoding='utf-8') as file:\n",
    "            writer = csv.writer(file, delimiter=',')\n",
    "            writer.writerow([name,discount,tag,original_price,discount_price,deadline])\n",
    "            \n",
    "finishTime = time.time()\n",
    "print(finishTime - startTime,'secs spend to craw these 3 pages')\n",
    "\n",
    "filechecker('sgp_4-6.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mutiple thread\n",
    "-3thread for crawlering the number 7-9 pages of switch game price info. in hottest order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swg_crawler(link_list,filename):\n",
    "    with open(filename,\"a+\",encoding='utf-8') as file:\n",
    "            writer = csv.writer(file, delimiter=',')\n",
    "            writer.writerow(['Name','Discount','Tag','Original_Price','Discount_Price','Deadline'])\n",
    "    \n",
    "    for link in link_list:\n",
    "        response=requests.get(link)\n",
    "        soup=BeautifulSoup(response.text,'html.parser')\n",
    "    \n",
    "        games=soup.find_all('div',{'class':'cell'})\n",
    "        games\n",
    "        for game in games:\n",
    "            describe=game.find('div',{'class':'h6 name'}).text.strip()\n",
    "            name=[]\n",
    "            discount=[]\n",
    "            tag=[]\n",
    "            original_price=[]\n",
    "            discount_price=[]\n",
    "            deadline=[]\n",
    "            try:\n",
    "                describe.split('\\n')[0]\n",
    "                name=describe.split('\\n')[0]\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                describe.split('\\n')[1]\n",
    "                discount=describe.split('\\n')[1]\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                describe.split('\\n')[2]\n",
    "                tag=describe.split('\\n')[2]\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                original_price=game.find('s',{'class':'text-muted'}).text.strip()\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                discount_price=game.find('strong').text.strip()\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                deadline=game.find('div',{'class':'w-100'}).find('small').text.strip()\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "            with open(filename,\"a+\",encoding='utf-8') as file:\n",
    "                writer = csv.writer(file, delimiter=',')\n",
    "                writer.writerow([name,discount,tag,original_price,discount_price,deadline])\n",
    "    \n",
    "    filechecker(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https://www.dekudeals.com/hottest?page=7', 'https://www.dekudeals.com/hottest?page=8', 'https://www.dekudeals.com/hottest?page=9'], ['https://www.dekudeals.com/hottest?page=10', 'https://www.dekudeals.com/hottest?page=11']]\n"
     ]
    }
   ],
   "source": [
    "link_list7=[]\n",
    "link_list9=[]\n",
    "link_list_large=[]\n",
    "for i in range(7,10):\n",
    "    link_list7.append(main+'hottest?page='+str(i))\n",
    "for i in range(10,12):\n",
    "    link_list9.append(main+'hottest?page='+str(i))\n",
    "\n",
    "link_list_large.append(link_list7)\n",
    "link_list_large.append(link_list9)\n",
    "\n",
    "print(link_list_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0003299713134765625 secs spend to craw these 5 pages\n"
     ]
    }
   ],
   "source": [
    "startTime=time.time()\n",
    "\n",
    "for link_list,n in zip(link_list_large,[7,9]):\n",
    "    _thread.start_new_thread(swg_crawler,(link_list,'sgp_{0}.csv'.format(str(n)) ))\n",
    "    \n",
    "finishTime = time.time()\n",
    "print(finishTime - startTime,'secs spend to craw these 5 pages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asynchronous Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp \n",
    "import asyncio \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swg_crawler_single(link,filename='asyncio.csv'):\n",
    "    response=requests.get(link)\n",
    "    soup=BeautifulSoup(response.text,'html.parser')\n",
    "    \n",
    "    games=soup.find_all('div',{'class':'cell'})\n",
    "    games\n",
    "    for game in games:\n",
    "        describe=game.find('div',{'class':'h6 name'}).text.strip()\n",
    "        name=[]\n",
    "        discount=[]\n",
    "        tag=[]\n",
    "        original_price=[]\n",
    "        discount_price=[]\n",
    "        deadline=[]\n",
    "        try:\n",
    "            describe.split('\\n')[0]\n",
    "            name=describe.split('\\n')[0]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            describe.split('\\n')[1]\n",
    "            discount=describe.split('\\n')[1]\n",
    "        except:\n",
    "              pass\n",
    "        try:\n",
    "            describe.split('\\n')[2]\n",
    "            tag=describe.split('\\n')[2]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            original_price=game.find('s',{'class':'text-muted'}).text.strip()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            discount_price=game.find('strong').text.strip()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            deadline=game.find('div',{'class':'w-100'}).find('small').text.strip()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        with open(filename,\"a+\",encoding='utf-8') as file:\n",
    "            writer = csv.writer(file, delimiter=',')\n",
    "            writer.writerow([name,discount,tag,original_price,discount_price,deadline])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Async total time :  13.381500005722046\n",
      "top 5 of 387 rows from asyncio.csv data\n",
      "                                           Name Discount                   Tag  \\\n",
      "0        Tales of Vesperia: Definitive Edition     -60%  Matches previous low   \n",
      "1  Mario + Rabbids Kingdom Battle Gold Edition     -75%  Matches previous low   \n",
      "2                      DRAGON BALL Xenoverse 2     -80%     Lowest price ever   \n",
      "3          Monster Hunter Generations Ultimate     -60%  Matches previous low   \n",
      "4                Trine 4: The Nightmare Prince     -60%     Lowest price ever   \n",
      "\n",
      "  Original_Price Discount_Price          Deadline  \n",
      "0         $49.99         $19.99  Sale ends May 26  \n",
      "1         $79.99         $19.99  Sale ends May 26  \n",
      "2         $49.99          $9.99  Sale ends May 26  \n",
      "3         $49.99         $19.99  Sale ends May 27  \n",
      "4         $29.99         $11.99  Sale ends May 26  \n"
     ]
    }
   ],
   "source": [
    "with open('asyncio.csv',\"a+\",encoding='utf-8') as file:\n",
    "            writer = csv.writer(file, delimiter=',')\n",
    "            writer.writerow(['Name','Discount','Tag','Original_Price','Discount_Price','Deadline'])\n",
    "\n",
    "main='https://www.dekudeals.com/'\n",
    "link_list=[]\n",
    "for i in range(1,12):\n",
    "    link_list.append(main+'hottest?page='+str(i)) \n",
    "    \n",
    "n=0\n",
    "async def job(link):                   # async 形式的功能\n",
    "    swg_crawler_single(link)\n",
    "    await asyncio.sleep(0)           #等待數秒並切換  #也可移除表不等待\n",
    "    \n",
    "\n",
    "async def main(loop):                       # async 形式的功能\n",
    "    tasks = [loop.create_task(job(link)) for link in link_list]    # 創建任務, 但是不執行\n",
    "    await asyncio.wait(tasks)               # 執行並等待所有任務完成\n",
    "\n",
    "startTime = time.time()\n",
    "loop = asyncio.get_event_loop()             # 建立 loop\n",
    "loop.run_until_complete(main(loop))         # 執行 loop                        \n",
    "#loop.close() #關閉 loop\n",
    "finishTime=time.time()\n",
    "print(\"Async total time : \", finishTime - startTime)\n",
    "filechecker('asyncio.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single time :  11.486860990524292\n",
      "top 5 of 387 rows from single.csv data\n",
      "                                           Name Discount                   Tag  \\\n",
      "0        Tales of Vesperia: Definitive Edition     -60%  Matches previous low   \n",
      "1  Mario + Rabbids Kingdom Battle Gold Edition     -75%  Matches previous low   \n",
      "2                      DRAGON BALL Xenoverse 2     -80%     Lowest price ever   \n",
      "3          Monster Hunter Generations Ultimate     -60%  Matches previous low   \n",
      "4                Trine 4: The Nightmare Prince     -60%     Lowest price ever   \n",
      "\n",
      "  Original_Price Discount_Price          Deadline  \n",
      "0         $49.99         $19.99  Sale ends May 26  \n",
      "1         $79.99         $19.99  Sale ends May 26  \n",
      "2         $49.99          $9.99  Sale ends May 26  \n",
      "3         $49.99         $19.99  Sale ends May 27  \n",
      "4         $29.99         $11.99  Sale ends May 26  \n"
     ]
    }
   ],
   "source": [
    "with open('single.csv',\"a+\",encoding='utf-8') as file:\n",
    "            writer = csv.writer(file, delimiter=',')\n",
    "            writer.writerow(['Name','Discount','Tag','Original_Price','Discount_Price','Deadline'])\n",
    "            \n",
    "main='https://www.dekudeals.com/'\n",
    "link_list=[]\n",
    "for i in range(1,12):\n",
    "    link_list.append(main+'hottest?page='+str(i)) \n",
    "\n",
    "startTime=time.time()    \n",
    "for link in link_list:\n",
    "    swg_crawler_single(link,filename='single.csv')\n",
    "    \n",
    "    \n",
    "finishTime=time.time()\n",
    "print(\"Single time : \", finishTime - startTime)\n",
    "filechecker('single.csv')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutiple thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.517220973968506 secs spend to craw these 11 pages\n",
      "top 5 of 387 rows from muti_thread.csv data\n",
      "                                       Name Discount                   Tag  \\\n",
      "0          The Escapists: Complete Edition     -80%  Matches previous low   \n",
      "1                      Super Mario Odyssey     -17%                    []   \n",
      "2                                Hard West     -90%  Matches previous low   \n",
      "3  The Legend of Zelda: Breath of the Wild     -18%                    []   \n",
      "4     Donkey Kong Country: Tropical Freeze     -17%                    []   \n",
      "\n",
      "  Original_Price Discount_Price          Deadline  \n",
      "0         $14.99          $2.99  Sale ends May 24  \n",
      "1         $59.99         $49.99                []  \n",
      "2         $19.99          $1.99  Sale ends May 27  \n",
      "3         $59.99         $49.49                []  \n",
      "4         $59.99         $49.99                []  \n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "with open('muti_thread.csv',\"a+\",encoding='utf-8') as file:\n",
    "            writer = csv.writer(file, delimiter=',')\n",
    "            writer.writerow(['Name','Discount','Tag','Original_Price','Discount_Price','Deadline'])\n",
    "\n",
    "main='https://www.dekudeals.com/'\n",
    "link_list=[]\n",
    "for i in range(1,12):\n",
    "    link_list.append(main+'hottest?page='+str(i)) \n",
    "\n",
    "\n",
    "startTime=time.time()\n",
    "\n",
    "threads=[]\n",
    "#build all the tasks \n",
    "for link in link_list:\n",
    "    thread = threading.Thread(target=swg_crawler_single,args=(link,'muti_thread.csv'))\n",
    "    threads.append(thread) \n",
    "    \n",
    "#start all the tasks at once\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "    \n",
    "#wait for all the tasks to finish\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "    \n",
    "finishTime = time.time()\n",
    "print(finishTime - startTime,'secs spend to craw these 11 pages')\n",
    "filechecker('muti_thread.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
